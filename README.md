# Reinforcement Learning with ViZDoom
Training bots with reinforcement learning techniques to survive the labyrinths of Doom! This project uses ViZDoom ([Wydmuch et al. 2018](https://arxiv.org/abs/1809.03470)) for managing setup and interaction of bots with the environment. I've started this primarily as an exploratory project for my own understanding of latest developments in RL. Implementation of several stuff is heavily inspired from the [tutorials](https://github.com/mwydmuch/ViZDoom/tree/master/examples) of ViZDoom.

## Models \[WIP]
Check out `agents.py` for implementations of the RL agents. Currently, the following are available.
  - **DQN**: A convolutional network which computes and updates Q-values for state-action pairs using experience replay.
  - **Double DQN**: Target Q-values are generated by a static network to prevent maximization bias observed for vanilla DQNs. 
 
## Usage Instructions
Clone this repository locally and execute the training shell command from within the repo directory. Settings related to the scenario and agent can be modified within the configuration files. For example to train a double DQN, run:

```
python3 main.py --config 'configs/double_dqn.yaml' --agent 'double_dqn' --task 'train' 
``` 

To watch a trained model's performance on the scenario, run (the trained model will be saved as `best_model.ckpt` inside the agent's directory in `outputs`):
  
```
python3 main.py --config 'configs/double_dqn.yaml' --agent 'double_dqn' --task 'watch' --load 'outputs/double_dqn/name_of_output_dir'
```
